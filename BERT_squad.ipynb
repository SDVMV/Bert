{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_squad",
      "provenance": [],
      "collapsed_sections": [
        "qDIlHd5Tos6C"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SDVMV/Bert/blob/master/BERT_squad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAUq7duyG5J0",
        "colab_type": "text"
      },
      "source": [
        "# Fase 1: Importar dependencias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XYlThAmGZg1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "492b998b-a361-4ff0-a991-5605582ca21b"
      },
      "source": [
        "!pip install sentencepiece\n",
        "#!pip install tf-models-official\n",
        "!pip install tf-models-nightly # mejor instalar la versión en desarrollo\n",
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 14.0MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 3.4MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting tf-models-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/d1/9084b3e7ef41802935644402a07a97ea70cd00cdb44702b89a0988f6376b/tf_models_nightly-2.3.0.dev20200822-py2.py3-none-any.whl (920kB)\n",
            "\u001b[K     |████████████████████████████████| 921kB 3.5MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/2a/496e06fd289c01dc21b11970be1261c87ce1cc22d5340c14b516160822a7/opencv_python_headless-4.4.0.42-cp36-cp36m-manylinux2014_x86_64.whl (36.6MB)\n",
            "\u001b[K     |████████████████████████████████| 36.6MB 110kB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (4.1.3)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.5.6)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.15.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.7)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f5/8e6e85ce2e9f6e05040cf0d4e26f43a4718bcc4bce988b433276d4b1a5c1/py-cpuinfo-7.0.0.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.18.5)\n",
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/d5/9dfb0ea7d129aa17a3ba29e3a3169a6c0d177077e109bfb6f2dc677227b5/tf_nightly-2.4.0.dev20200822-cp36-cp36m-manylinux2010_x86_64.whl (345.4MB)\n",
            "\u001b[K     |████████████████████████████████| 345.4MB 12kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.2.2)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.3.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.7.12)\n",
            "Collecting tensorflow-model-optimization>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/cc/4b0831f492396f03a4563cc749ad94cbf7af986aaa7a7d89e5979029ce5c/tensorflow_model_optimization-0.4.1-py2.py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 58.9MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.0.5)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.29.21)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (3.13)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (2.0.1)\n",
            "Collecting tf-slim>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n",
            "\u001b[K     |████████████████████████████████| 358kB 55.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.1.91)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (0.8.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (7.0.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (5.4.8)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from tf-models-nightly) (1.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (4.6)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->tf-models-nightly) (0.4.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (4.41.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->tf-models-nightly) (2020.6.20)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->tf-models-nightly) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.3.3)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/d7/42d45fc742723a3963a4def67b16322eeacc0e28881e683f62bc402f2b2d/tf_estimator_nightly-2.4.0.dev2020082201-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 56.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (2.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.34.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.6.3)\n",
            "Collecting tb-nightly<3.0.0a0,>=2.4.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/a5/d7f0ae0fa63f2b0b2ddeb3f420bf5f90f8f9c986c1b5729dec8b2e928246/tb_nightly-2.4.0a20200822-py3-none-any.whl (8.9MB)\n",
            "\u001b[K     |████████████████████████████████| 8.9MB 57.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (3.7.4.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.31.0)\n",
            "Collecting flatbuffers>=1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly->tf-models-nightly) (1.1.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tf-models-nightly) (0.10.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (0.0.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->tf-models-nightly) (1.17.2)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-optimization>=0.2.1->tf-models-nightly) (0.1.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->tf-models-nightly) (2.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->tf-models-nightly) (2018.9)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons->tf-models-nightly) (2.7.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.3.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (0.22.2)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->tf-models-nightly) (19.3.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->tf-models-nightly) (49.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle>=1.3.9->tf-models-nightly) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-nightly) (1.3)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->tf-models-nightly) (1.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (3.2.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.6.7->tf-models-nightly) (4.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-nightly) (1.52.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly->tf-models-nightly) (3.1.0)\n",
            "Building wheels for collected packages: py-cpuinfo, seqeval\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-7.0.0-cp36-none-any.whl size=20069 sha256=307e7da87edb5bb5b01a700be692229dc238e1e22140cdf6862d922124ed449b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/93/7b/127daf0c3a5a49feb2fecd468d508067c733fba5192f726ad1\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=b493f4a15a42154ca4e86ffe20ac53fdd50593aff59f627c5950034b5e8f4374\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built py-cpuinfo seqeval\n",
            "Installing collected packages: opencv-python-headless, py-cpuinfo, tf-estimator-nightly, tb-nightly, flatbuffers, tf-nightly, tensorflow-model-optimization, seqeval, tf-slim, tf-models-nightly\n",
            "Successfully installed flatbuffers-1.12 opencv-python-headless-4.4.0.42 py-cpuinfo-7.0.0 seqeval-0.0.12 tb-nightly-2.4.0a20200822 tensorflow-model-optimization-0.4.1 tf-estimator-nightly-2.4.0.dev2020082201 tf-models-nightly-2.3.0.dev20200822 tf-nightly-2.4.0.dev20200822 tf-slim-1.1.0\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.6/dist-packages (2.4.0.dev20200822)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.31.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0.dev2020082201)\n",
            "Requirement already satisfied: tb-nightly<3.0.0a0,>=2.4.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.4.0a20200822)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (49.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<3.0.0a0,>=2.4.0a0->tf-nightly) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDdYqvxPHnI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFS9XSASHvQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "outputId": "f9859e9f-5200-4999-ed0d-026c3f45c3cd"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0-dev20200822'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv38eJzSH00S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "from official.nlp.bert.tokenization import FullTokenizer\n",
        "from official.nlp.bert.input_pipeline import create_squad_dataset\n",
        "from official.nlp.data.squad_lib import generate_tf_record_from_json_file\n",
        "\n",
        "from official.nlp import optimization\n",
        "\n",
        "from official.nlp.data.squad_lib import read_squad_examples\n",
        "from official.nlp.data.squad_lib import FeatureWriter\n",
        "from official.nlp.data.squad_lib import convert_examples_to_features\n",
        "from official.nlp.data.squad_lib import write_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjzrCZJMJN1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import json\n",
        "import collections\n",
        "import os\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSGw1I_zHAb5",
        "colab_type": "text"
      },
      "source": [
        "# Fase 2: Preprocesado de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfdTEReCKFky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "d00831d9-ef06-4e1f-8993-1d1114ad4f4b"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_PcOeemKcYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_meta_data = generate_tf_record_from_json_file(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.json\",\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/vocab.txt\",\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.tf_record\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XQGdnANLziM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.io.gfile.GFile(\"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train_meta_data\", \"w\") as writer:\n",
        "    writer.write(json.dumps(input_meta_data, indent=4) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "295nG2zQMYSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "train_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/train-v1.1.tf_record\",\n",
        "    input_meta_data['max_seq_length'], # 384\n",
        "    BATCH_SIZE,\n",
        "    is_training=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4939RJqHRUs",
        "colab_type": "text"
      },
      "source": [
        "# Fase 3: Construcción del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcQcFi8kOc6K",
        "colab_type": "text"
      },
      "source": [
        "## Capa Squad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjmLeQjiaOo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSquadLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BertSquadLayer, self).__init__()\n",
        "    self.final_dense = tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    logits = self.final_dense(inputs) # (batch_size, seq_len, 2)\n",
        "\n",
        "    logits = tf.transpose(logits, [2, 0, 1]) # (2, batch_size, seq_len)\n",
        "    unstacked_logits = tf.unstack(logits, axis=0) # [(batch_size, seq_len), (batch_size, seq_len)] \n",
        "    return unstacked_logits[0], unstacked_logits[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQbQFKjUOeyf",
        "colab_type": "text"
      },
      "source": [
        "## Modelo completo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkIFb1GMT81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BERTSquad(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 name=\"bert_squad\"):\n",
        "        super(BERTSquad, self).__init__(name=name)\n",
        "        \n",
        "        self.bert_layer = hub.KerasLayer(\n",
        "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "            trainable=True)\n",
        "        \n",
        "        self.squad_layer = BertSquadLayer()\n",
        "    \n",
        "    def apply_bert(self, inputs):\n",
        "        _ , sequence_output = self.bert_layer([inputs[\"input_word_ids\"],\n",
        "                                               inputs[\"input_mask\"],\n",
        "                                               inputs[\"input_type_ids\"]])\n",
        "        return sequence_output\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_output = self.apply_bert(inputs)\n",
        "\n",
        "        start_logits, end_logits = self.squad_layer(seq_output)\n",
        "        \n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBmSU2RnHV_a",
        "colab_type": "text"
      },
      "source": [
        "# Fase 4: Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnA3WHwRIHAZ",
        "colab_type": "text"
      },
      "source": [
        "## Creación de la IA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEUxomxENRoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_DATA_SIZE = 88641\n",
        "NB_BATCHES_TRAIN = 2000\n",
        "BATCH_SIZE = 4\n",
        "NB_EPOCHS = 3\n",
        "INIT_LR = 5e-5\n",
        "WARMUP_STEPS = int(NB_BATCHES_TRAIN * 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pg6EKe2daFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_light = train_dataset.take(NB_BATCHES_TRAIN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHd5MzTdNIZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_squad = BERTSquad()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0cvDjBm_KXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optimization.create_optimizer(\n",
        "    init_lr=INIT_LR,\n",
        "    num_train_steps=NB_BATCHES_TRAIN,\n",
        "    num_warmup_steps=WARMUP_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6kG-HpzVK7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squad_loss_fn(labels, model_outputs):\n",
        "    start_positions = labels['start_positions']\n",
        "    end_positions = labels['end_positions']\n",
        "    start_logits, end_logits = model_outputs\n",
        "\n",
        "    start_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        start_positions, start_logits, from_logits=True)\n",
        "    end_loss = tf.keras.backend.sparse_categorical_crossentropy(\n",
        "        end_positions, end_logits, from_logits=True)\n",
        "    \n",
        "    total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_C_WA5R_Cb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "284fc4ce-2ce2-4b13-b6b7-350614310565"
      },
      "source": [
        "next(iter(train_dataset_light))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'input_mask': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0],\n",
              "         [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_type_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0],\n",
              "         [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
              "  'input_word_ids': <tf.Tensor: shape=(4, 384), dtype=int32, numpy=\n",
              "  array([[ 101, 2054, 3815, ...,    0,    0,    0],\n",
              "         [ 101, 2129, 2116, ...,    0,    0,    0],\n",
              "         [ 101, 1999, 2054, ...,    0,    0,    0],\n",
              "         [ 101, 2000, 3183, ...,    0,    0,    0]], dtype=int32)>},\n",
              " {'end_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([73, 25, 35, 93], dtype=int32)>,\n",
              "  'start_positions': <tf.Tensor: shape=(4,), dtype=int32, numpy=array([71, 23, 35, 90], dtype=int32)>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-iE2QFC_KRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_squad.compile(optimizer,\n",
        "                   squad_loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjp-_4OyTbuK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f85640d4-9adf-44f9-b612-5c414e085077"
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(bert_squad=bert_squad)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Último checkpoint restaurado!!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Último checkpoint restaurado!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDgEq09xIOOl",
        "colab_type": "text"
      },
      "source": [
        "## Entrenamiento personalizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ywelW3uaSbT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c27881e-7ebe-438e-b6dd-6830bd4bcc79"
      },
      "source": [
        "for epoch in range(NB_EPOCHS):\n",
        "    print(\"Inicio del Epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    \n",
        "    for (batch, (inputs, targets)) in enumerate(train_dataset_light):\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_outputs = bert_squad(inputs)\n",
        "            loss = squad_loss_fn(targets, model_outputs)\n",
        "        \n",
        "        gradients = tape.gradient(loss, bert_squad.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, bert_squad.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Lote {} Pérdida {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result()))\n",
        "        \n",
        "        if batch % 500 == 0:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print(\"Guardando checkpoint para el epoch {} en el directorio {}\".format(epoch+1,\n",
        "                                                                ckpt_save_path))\n",
        "    print(\"Tiempo total para entrenar 1 epoch: {} segs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicio del Epoch 1\n",
            "Epoch 1 Lote 0 Pérdida 6.0621\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-1\n",
            "Epoch 1 Lote 50 Pérdida 5.8209\n",
            "Epoch 1 Lote 100 Pérdida 5.2858\n",
            "Epoch 1 Lote 150 Pérdida 4.6081\n",
            "Epoch 1 Lote 200 Pérdida 4.1292\n",
            "Epoch 1 Lote 250 Pérdida 3.7406\n",
            "Epoch 1 Lote 300 Pérdida 3.4795\n",
            "Epoch 1 Lote 350 Pérdida 3.3128\n",
            "Epoch 1 Lote 400 Pérdida 3.1128\n",
            "Epoch 1 Lote 450 Pérdida 2.9574\n",
            "Epoch 1 Lote 500 Pérdida 2.8129\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-2\n",
            "Epoch 1 Lote 550 Pérdida 2.6840\n",
            "Epoch 1 Lote 600 Pérdida 2.6034\n",
            "Epoch 1 Lote 650 Pérdida 2.5247\n",
            "Epoch 1 Lote 700 Pérdida 2.4478\n",
            "Epoch 1 Lote 750 Pérdida 2.3795\n",
            "Epoch 1 Lote 800 Pérdida 2.3212\n",
            "Epoch 1 Lote 850 Pérdida 2.2794\n",
            "Epoch 1 Lote 900 Pérdida 2.2269\n",
            "Epoch 1 Lote 950 Pérdida 2.1805\n",
            "Epoch 1 Lote 1000 Pérdida 2.1184\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-3\n",
            "Epoch 1 Lote 1050 Pérdida 2.0712\n",
            "Epoch 1 Lote 1100 Pérdida 2.0192\n",
            "Epoch 1 Lote 1150 Pérdida 1.9696\n",
            "Epoch 1 Lote 1200 Pérdida 1.9270\n",
            "Epoch 1 Lote 1250 Pérdida 1.9186\n",
            "Epoch 1 Lote 1300 Pérdida 1.8983\n",
            "Epoch 1 Lote 1350 Pérdida 1.8858\n",
            "Epoch 1 Lote 1400 Pérdida 1.8687\n",
            "Epoch 1 Lote 1450 Pérdida 1.8431\n",
            "Epoch 1 Lote 1500 Pérdida 1.8203\n",
            "Guardando checkpoint para el epoch 1 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-4\n",
            "Epoch 1 Lote 1550 Pérdida 1.8009\n",
            "Epoch 1 Lote 1600 Pérdida 1.7833\n",
            "Epoch 1 Lote 1650 Pérdida 1.7777\n",
            "Epoch 1 Lote 1700 Pérdida 1.7699\n",
            "Epoch 1 Lote 1750 Pérdida 1.7521\n",
            "Epoch 1 Lote 1800 Pérdida 1.7437\n",
            "Epoch 1 Lote 1850 Pérdida 1.7187\n",
            "Epoch 1 Lote 1900 Pérdida 1.6900\n",
            "Epoch 1 Lote 1950 Pérdida 1.6774\n",
            "Tiempo total para entrenar 1 epoch: 5262.5173897743225 segs\n",
            "\n",
            "Inicio del Epoch 2\n",
            "Epoch 2 Lote 0 Pérdida 1.1600\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-5\n",
            "Epoch 2 Lote 50 Pérdida 1.1323\n",
            "Epoch 2 Lote 100 Pérdida 1.1650\n",
            "Epoch 2 Lote 150 Pérdida 1.0600\n",
            "Epoch 2 Lote 200 Pérdida 1.0460\n",
            "Epoch 2 Lote 250 Pérdida 0.9981\n",
            "Epoch 2 Lote 300 Pérdida 0.9548\n",
            "Epoch 2 Lote 350 Pérdida 0.9416\n",
            "Epoch 2 Lote 400 Pérdida 0.8956\n",
            "Epoch 2 Lote 450 Pérdida 0.8426\n",
            "Epoch 2 Lote 500 Pérdida 0.8125\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-6\n",
            "Epoch 2 Lote 550 Pérdida 0.7875\n",
            "Epoch 2 Lote 600 Pérdida 0.7739\n",
            "Epoch 2 Lote 650 Pérdida 0.7629\n",
            "Epoch 2 Lote 700 Pérdida 0.7346\n",
            "Epoch 2 Lote 750 Pérdida 0.7113\n",
            "Epoch 2 Lote 800 Pérdida 0.6930\n",
            "Epoch 2 Lote 850 Pérdida 0.6815\n",
            "Epoch 2 Lote 900 Pérdida 0.6704\n",
            "Epoch 2 Lote 950 Pérdida 0.6603\n",
            "Epoch 2 Lote 1000 Pérdida 0.6411\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-7\n",
            "Epoch 2 Lote 1050 Pérdida 0.6289\n",
            "Epoch 2 Lote 1100 Pérdida 0.6145\n",
            "Epoch 2 Lote 1150 Pérdida 0.6004\n",
            "Epoch 2 Lote 1200 Pérdida 0.5895\n",
            "Epoch 2 Lote 1250 Pérdida 0.5867\n",
            "Epoch 2 Lote 1300 Pérdida 0.5894\n",
            "Epoch 2 Lote 1350 Pérdida 0.6004\n",
            "Epoch 2 Lote 1400 Pérdida 0.5978\n",
            "Epoch 2 Lote 1450 Pérdida 0.5938\n",
            "Epoch 2 Lote 1500 Pérdida 0.5948\n",
            "Guardando checkpoint para el epoch 2 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-8\n",
            "Epoch 2 Lote 1550 Pérdida 0.5932\n",
            "Epoch 2 Lote 1600 Pérdida 0.5958\n",
            "Epoch 2 Lote 1650 Pérdida 0.6041\n",
            "Epoch 2 Lote 1700 Pérdida 0.6120\n",
            "Epoch 2 Lote 1750 Pérdida 0.6117\n",
            "Epoch 2 Lote 1800 Pérdida 0.6166\n",
            "Epoch 2 Lote 1850 Pérdida 0.6114\n",
            "Epoch 2 Lote 1900 Pérdida 0.6073\n",
            "Epoch 2 Lote 1950 Pérdida 0.6167\n",
            "Tiempo total para entrenar 1 epoch: 5323.075678825378 segs\n",
            "\n",
            "Inicio del Epoch 3\n",
            "Epoch 3 Lote 0 Pérdida 1.6725\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-9\n",
            "Epoch 3 Lote 50 Pérdida 1.1375\n",
            "Epoch 3 Lote 100 Pérdida 1.0644\n",
            "Epoch 3 Lote 150 Pérdida 1.0579\n",
            "Epoch 3 Lote 200 Pérdida 1.0375\n",
            "Epoch 3 Lote 250 Pérdida 1.0067\n",
            "Epoch 3 Lote 300 Pérdida 0.9554\n",
            "Epoch 3 Lote 350 Pérdida 0.9430\n",
            "Epoch 3 Lote 400 Pérdida 0.8927\n",
            "Epoch 3 Lote 450 Pérdida 0.8453\n",
            "Epoch 3 Lote 500 Pérdida 0.8164\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-10\n",
            "Epoch 3 Lote 550 Pérdida 0.7860\n",
            "Epoch 3 Lote 600 Pérdida 0.7754\n",
            "Epoch 3 Lote 650 Pérdida 0.7596\n",
            "Epoch 3 Lote 700 Pérdida 0.7369\n",
            "Epoch 3 Lote 750 Pérdida 0.7108\n",
            "Epoch 3 Lote 800 Pérdida 0.6945\n",
            "Epoch 3 Lote 850 Pérdida 0.6831\n",
            "Epoch 3 Lote 900 Pérdida 0.6735\n",
            "Epoch 3 Lote 950 Pérdida 0.6600\n",
            "Epoch 3 Lote 1000 Pérdida 0.6426\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-11\n",
            "Epoch 3 Lote 1050 Pérdida 0.6262\n",
            "Epoch 3 Lote 1100 Pérdida 0.6146\n",
            "Epoch 3 Lote 1150 Pérdida 0.6007\n",
            "Epoch 3 Lote 1200 Pérdida 0.5895\n",
            "Epoch 3 Lote 1250 Pérdida 0.5879\n",
            "Epoch 3 Lote 1300 Pérdida 0.5926\n",
            "Epoch 3 Lote 1350 Pérdida 0.6027\n",
            "Epoch 3 Lote 1400 Pérdida 0.6001\n",
            "Epoch 3 Lote 1450 Pérdida 0.5950\n",
            "Epoch 3 Lote 1500 Pérdida 0.5975\n",
            "Guardando checkpoint para el epoch 3 en el directorio ./drive/My Drive/Curso de NLP/BERT/ckpt_bert_squad/ckpt-12\n",
            "Epoch 3 Lote 1550 Pérdida 0.5964\n",
            "Epoch 3 Lote 1600 Pérdida 0.5967\n",
            "Epoch 3 Lote 1650 Pérdida 0.6051\n",
            "Epoch 3 Lote 1700 Pérdida 0.6109\n",
            "Epoch 3 Lote 1750 Pérdida 0.6126\n",
            "Epoch 3 Lote 1800 Pérdida 0.6146\n",
            "Epoch 3 Lote 1850 Pérdida 0.6134\n",
            "Epoch 3 Lote 1900 Pérdida 0.6089\n",
            "Epoch 3 Lote 1950 Pérdida 0.6163\n",
            "Tiempo total para entrenar 1 epoch: 5372.474010705948 segs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6WquTCiIR7t",
        "colab_type": "text"
      },
      "source": [
        "# Fase 5: Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIlHd5Tos6C",
        "colab_type": "text"
      },
      "source": [
        "## Preparación de la evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7_AyTIpTTJ",
        "colab_type": "text"
      },
      "source": [
        "Get the dev set in the session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGKl84s5WrhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_examples = read_squad_examples(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/dev-v1.1.json\",\n",
        "    is_training=False,\n",
        "    version_2_with_negative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAEUcZDSpYLD",
        "colab_type": "text"
      },
      "source": [
        "Define the function that will write the tf_record file for the dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCVmIgnEo83o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_writer = FeatureWriter(\n",
        "    filename=os.path.join(\"/content/drive/My Drive/Curso de NLP/BERT/squad_data/\",\n",
        "                          \"eval.tf_record\"),\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8aSLFdmp71I",
        "colab_type": "text"
      },
      "source": [
        "Create a tokenizer for future information needs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH5exQ7KwnuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHudjJ_qAzo",
        "colab_type": "text"
      },
      "source": [
        "Define the function that add the features (feature is a protocol in tensorflow) to our eval_features list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmQ5GtoTxRjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _append_feature(feature, is_padding):\n",
        "    if not is_padding:\n",
        "        eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLAcwCiaqHi_",
        "colab_type": "text"
      },
      "source": [
        "Create the eval features and the writes the tf.record file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7kGYmUwGQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_features = []\n",
        "dataset_size = convert_examples_to_features(\n",
        "    examples=eval_examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=384,\n",
        "    doc_stride=128,\n",
        "    max_query_length=64,\n",
        "    is_training=False,\n",
        "    output_fn=_append_feature,\n",
        "    batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZfwPEwMabx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbKhx3zuq844",
        "colab_type": "text"
      },
      "source": [
        "Load the ready-to-be-used dataset to our session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUqYvG5TxctF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "eval_dataset = create_squad_dataset(\n",
        "    \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/eval.tf_record\",\n",
        "    384,#input_meta_data['max_seq_length'],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRbKrFYoo8e8",
        "colab_type": "text"
      },
      "source": [
        "## Llevar a cabo las prediccioness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyckEWDbrLEX",
        "colab_type": "text"
      },
      "source": [
        "Definir un cierto tipo de colección (como un diccionario)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyWOUKaDP0H0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28abKVvqrRa4",
        "colab_type": "text"
      },
      "source": [
        "Devuelve cada elemento del lote de salida, uno por uno."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScaA0SZQgQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_raw_results(predictions):\n",
        "    for unique_ids, start_logits, end_logits in zip(predictions['unique_ids'],\n",
        "                                                    predictions['start_logits'],\n",
        "                                                    predictions['end_logits']):\n",
        "        yield RawResult(\n",
        "            unique_id=unique_ids.numpy(),\n",
        "            start_logits=start_logits.numpy().tolist(),\n",
        "            end_logits=end_logits.numpy().tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLOOmnLre5C",
        "colab_type": "text"
      },
      "source": [
        "Hacemos nuestras predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqD78Xdjrvpn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b128075d-bfc1-4df6-852e-f1020bbc404f"
      },
      "source": [
        "all_results = []\n",
        "for count, inputs in enumerate(eval_dataset):\n",
        "    x, _ = inputs  \n",
        "    unique_ids = x.pop(\"unique_ids\")\n",
        "    start_logits, end_logits = bert_squad(x, training=False)\n",
        "    output_dict = dict(\n",
        "        unique_ids=unique_ids,\n",
        "        start_logits=start_logits,\n",
        "        end_logits=end_logits)\n",
        "    for result in get_raw_results(output_dict):\n",
        "        all_results.append(result)\n",
        "    if count % 100 == 0:\n",
        "        print(\"{}/{}\".format(count, 2709))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/2709\n",
            "100/2709\n",
            "200/2709\n",
            "300/2709\n",
            "400/2709\n",
            "500/2709\n",
            "600/2709\n",
            "700/2709\n",
            "800/2709\n",
            "900/2709\n",
            "1000/2709\n",
            "1100/2709\n",
            "1200/2709\n",
            "1300/2709\n",
            "1400/2709\n",
            "1500/2709\n",
            "1600/2709\n",
            "1700/2709\n",
            "1800/2709\n",
            "1900/2709\n",
            "2000/2709\n",
            "2100/2709\n",
            "2200/2709\n",
            "2300/2709\n",
            "2400/2709\n",
            "2500/2709\n",
            "2600/2709\n",
            "2700/2709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQ6kIqGriHr",
        "colab_type": "text"
      },
      "source": [
        "Escribimos nuestras predicciones en un fichero JSON que funcionará con el script de evaluación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esLdRf7uM3Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_prediction_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/predictions.json\"\n",
        "output_nbest_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/nbest_predictions.json\"\n",
        "output_null_log_odds_file = \"/content/drive/My Drive/Curso de NLP/BERT/squad_data/null_odds.json\"\n",
        "\n",
        "write_predictions(\n",
        "    eval_examples,\n",
        "    eval_features,\n",
        "    all_results,\n",
        "    20,\n",
        "    30,\n",
        "    True,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eaIHyDIYHHx",
        "colab_type": "text"
      },
      "source": [
        "## Predicción casera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0F4l5h8Zdha",
        "colab_type": "text"
      },
      "source": [
        "### Creación del diccionario de entrada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrhHzx7ycXXo",
        "colab_type": "text"
      },
      "source": [
        "Concatenamos la pregunta y el contexto, separados por `[\"SEP\"]`, tras la tokenización, tal cual como lo hicimos con el conjunto de entrenamiento.\n",
        "\n",
        "Lo importante a recordar es que queremos que nuestra respuesta empiece y termine con una palabra real. Por ejemplo, la palabra \"ecologically\" es tokenizada como `[\"ecological\", \"##ly\"]`, y si el token de fin es `[\"ecological\"]` queremos usar la palabra \"ecologically\" como palabra final (del mismo modo si el token de fin es`[\"##ly\"]`). Por eso, empezamos dividiendo nuestro contexto en palabras, y luego pasamos a tokens, recordando qué token se corresponde con qué palabra (ver la función `tokenize_context()` para más detalle)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tuNXt98Zm4u",
        "colab_type": "text"
      },
      "source": [
        "#### Útiles varios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBjGoQ_wfmml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_bert_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "    trainable=False)\n",
        "vocab_file = my_bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = my_bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_fCe_hLC12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_whitespace(c):\n",
        "    '''\n",
        "    Indica si un cadena de caracteres se corresponde con un espacio en blanco / separador o no.\n",
        "    '''\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZA4TYnxLGVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def whitespace_split(text):\n",
        "    '''\n",
        "    Toma el texto y devuelve una lista de \"palabras\" separadas segun los \n",
        "    espacios en blanco / separadores anteriores.\n",
        "    '''\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    for c in text:\n",
        "        if is_whitespace(c):\n",
        "            prev_is_whitespace = True\n",
        "        else:\n",
        "            if prev_is_whitespace:\n",
        "                doc_tokens.append(c)\n",
        "            else:\n",
        "                doc_tokens[-1] += c\n",
        "            prev_is_whitespace = False\n",
        "    return doc_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fsfzt3GUNWQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_context(text_words):\n",
        "    '''\n",
        "    Toma una lista de palabras (devueltas por whitespace_split()) y tokeniza cada\n",
        "    palabra una por una. También almacena, para cada nuevo token, la palabra original\n",
        "    del parámetro text_words.\n",
        "    '''\n",
        "    text_tok = []\n",
        "    tok_to_word_id = []\n",
        "    for word_id, word in enumerate(text_words):\n",
        "        word_tok = tokenizer.tokenize(word)\n",
        "        text_tok += word_tok\n",
        "        tok_to_word_id += [word_id]*len(word_tok)\n",
        "    return text_tok, tok_to_word_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8qreqEURUOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ids(tokens):\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def get_mask(tokens):\n",
        "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
        "\n",
        "def get_segments(tokens):\n",
        "    seg_ids = []\n",
        "    current_seg_id = 0\n",
        "    for tok in tokens:\n",
        "        seg_ids.append(current_seg_id)\n",
        "        if tok == \"[SEP]\":\n",
        "            current_seg_id = 1-current_seg_id # Convierte 1 en 0 y viceversa\n",
        "    return seg_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2sPGXxsYUsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input_dict(question, context):\n",
        "    '''\n",
        "    Take a question and a context as strings and return a dictionary with the 3\n",
        "    elements needed for the model. Also return the context_words, the\n",
        "    context_tok to context_word ids correspondance and the length of\n",
        "    question_tok that we will need later.\n",
        "    '''\n",
        "    question_tok = tokenizer.tokenize(my_question)\n",
        "\n",
        "    context_words = whitespace_split(context)\n",
        "    context_tok, context_tok_to_word_id = tokenize_context(context_words)\n",
        "\n",
        "    input_tok = question_tok + [\"[SEP]\"] + context_tok + [\"[SEP]\"]\n",
        "    input_tok += [\"[PAD]\"]*(384-len(input_tok)) # in our case the model has been\n",
        "                                                # trained to have inputs of length max 384\n",
        "    input_dict = {}\n",
        "    input_dict[\"input_word_ids\"] = tf.expand_dims(tf.cast(get_ids(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_mask\"] = tf.expand_dims(tf.cast(get_mask(input_tok), tf.int32), 0)\n",
        "    input_dict[\"input_type_ids\"] = tf.expand_dims(tf.cast(get_segments(input_tok), tf.int32), 0)\n",
        "\n",
        "    return input_dict, context_words, context_tok_to_word_id, len(question_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAnaCZWTZpWT",
        "colab_type": "text"
      },
      "source": [
        "#### Creación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQlM-M8rMklA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_context = '''Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL_cE-o0U8mx",
        "colab_type": "text"
      },
      "source": [
        "Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor (worker, capitalist/business owner, landlord). Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeB29SCNNQ1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#my_question = '''What philosophy of thought addresses wealth inequality?'''\n",
        "my_question = '''What are examples of economic actors?'''\n",
        "#my_question = '''In a market economy, what is inequality a reflection of?'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v5nLMNjZufe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT066rMtZ65X",
        "colab_type": "text"
      },
      "source": [
        "### Predicción"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcJgDa9gVShl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhdGlIo5Z9IZ",
        "colab_type": "text"
      },
      "source": [
        "### Interpretación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQJMBkVLd9wp",
        "colab_type": "text"
      },
      "source": [
        "We remove the ids corresponding to the question and the `[\"SEP\"]` token:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfwBJfsSTwRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_logits_context = start_logits.numpy()[0, question_tok_len+1:]\n",
        "end_logits_context = end_logits.numpy()[0, question_tok_len+1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te1u6iZAawYf",
        "colab_type": "text"
      },
      "source": [
        "First easy interpretation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQJ8tp-1WvI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_word_id = context_tok_to_word_id[np.argmax(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[np.argmax(end_logits_context)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3PHA84rarse",
        "colab_type": "text"
      },
      "source": [
        "\"Advanced\" - making sure that the start of the answer is before the end:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiFZ2fUiRU_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9KEiFHPXXeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJDBL8KKa6NP",
        "colab_type": "text"
      },
      "source": [
        "Final answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Y3WDz0XwAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff600bb-528d-4b14-f800-731aa4448e9a"
      },
      "source": [
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "print(\"The answer to:\\n\" + my_question + \"\\nis:\\n\" + predicted_answer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The answer to:\n",
            "What are examples of economic actors?\n",
            "is:\n",
            "(worker, capitalist/business owner, landlord).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYGSk_5OSYUk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "dbd02f58-62e6-45c8-8ddd-b2b6989a2385"
      },
      "source": [
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE EXAMPLES OF ECONOMIC ACTORS?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> Neoclassical economics views inequalities in the distribution of income as arising from differences in value added by labor, capital and land. Within labor income distribution is due to differences in value added by different classifications of workers. In this perspective, wages and profits are determined by the marginal value added of each economic actor <mark>(worker, capitalist/business owner, landlord).</mark> Thus, in a market economy, inequality is a reflection of the productivity gap between highly-paid professions and lower-paid professions. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXMJwopGRzM",
        "colab_type": "text"
      },
      "source": [
        "#### Reto Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brg0AXkA0r74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "27b814f9-65bf-4ae0-90d7-c246b6363ba6"
      },
      "source": [
        "my_context = '''\n",
        "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
        "Common symptoms include fever, cough, fatigue, shortness of breath, and loss of smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
        "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces.'''\n",
        "\n",
        "my_question = '''What are the common symptoms of the disease?'''\n",
        "\n",
        "my_input_dict, my_context_words, context_tok_to_word_id, question_tok_len = create_input_dict(my_question, my_context)\n",
        "\n",
        "start_logits, end_logits = bert_squad(my_input_dict, training=False)\n",
        "\n",
        "pair_scores = np.ones((len(start_logits_context), len(end_logits_context)))*(-1E10)\n",
        "for i in range(len(start_logits_context-1)):\n",
        "    for j in range(i, len(end_logits_context)):\n",
        "        pair_scores[i, j] = start_logits_context[i] + end_logits_context[j]\n",
        "pair_scores_argmax = np.argmax(pair_scores)\n",
        "\n",
        "start_word_id = context_tok_to_word_id[pair_scores_argmax // len(start_logits_context)]\n",
        "end_word_id = context_tok_to_word_id[pair_scores_argmax % len(end_logits_context)]\n",
        "\n",
        "predicted_answer = ' '.join(my_context_words[start_word_id:end_word_id+1])\n",
        "\n",
        "\n",
        "from IPython.core.display import HTML\n",
        "display(HTML(f'<h2>{my_question.upper()}</h2>'))\n",
        "marked_text = str(my_context.replace(predicted_answer, f\"<mark>{predicted_answer}</mark>\"))\n",
        "display(HTML(f\"\"\"<blockquote> {marked_text} </blockquote>\"\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h2>WHAT ARE THE COMMON SYMPTOMS OF THE DISEASE?</h2>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<blockquote> \n",
              "Coronavirus disease 2019 is an infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in December 2019 in Wuhan, Hubei, China, and has resulted in an ongoing pandemic.\n",
              "Common symptoms include fever, cough, fatigue, <mark>shortness of breath, and loss of</mark> smell and taste.While most people have mild symptoms, some people develop acute respiratory distress syndrome (ARDS) possibly precipitated by cytokine storm, multi-organ failure, septic shock, and blood clots. The time from exposure to onset of symptoms is typically around five days, but may range from two to fourteen days.\n",
              "The virus is spread primarily via nose and mouth secretions including small droplets produced by coughing,[a] sneezing, and talking. The droplets usually do not travel through air over long distances. However, those standing in close proximity may inhale these droplets and become infected.[b] People may also become infected by touching a contaminated surface and then touching their face. The transmission may also occur through smaller droplets that are able to stay suspended in the air for longer periods of time in enclosed spaces. </blockquote>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fTt1zl8Fbom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}